{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "import math\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. A neural network class (feedforward, fully connected)\n",
    "\n",
    "Architectures are configurable. However, it only supports Stochastic Gradient Descent training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_deriv(x):\n",
    "    return x*(1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_pred, y_true):\n",
    "    return np.sum((y_pred - y_true)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet(object):\n",
    "    \n",
    "    def __init__(self, input_dim, layers_dim):\n",
    "        # store problem metadata\n",
    "        self.layers_dim = layers_dim\n",
    "        self.input_dim = input_dim\n",
    "        # keep track of errors - to make sure they're goig down\n",
    "        self.errors = [math.inf]\n",
    "        self.predictions = []\n",
    "        \"\"\"\n",
    "        Initialize weights\n",
    "        \"\"\"\n",
    "        self.weights = []\n",
    "        # get number of neurons in first hidden layer\n",
    "        k = layers_dim[0]\n",
    "        # initialize first weight matrix\n",
    "        W1 = np.random.random((k, input_dim))\n",
    "        self.weights.append(W1)\n",
    "        # Add the rest of the dimensions\n",
    "        for i in range(len(layers_dim) - 1):\n",
    "            # previous and post dimension\n",
    "            prev_dim, next_dim = layers_dim[i], layers_dim[i+1]\n",
    "            self.weights.append(np.random.random((next_dim, prev_dim + 1)))\n",
    "        \n",
    "        \"\"\"\n",
    "        Keep track of the partial derivatives with regards to each weight. \n",
    "        We will build them up using batch gradient descent\n",
    "        \"\"\"\n",
    "        self.derivatives = []\n",
    "        for weight in self.weights:\n",
    "            self.derivatives.append(np.zeros(weight.shape))\n",
    "        \n",
    "        \"\"\"\n",
    "        Keep track of the activations. \n",
    "        \"\"\"\n",
    "        self.activations = []\n",
    "        \"\"\"\n",
    "        Keep track of the errors (delta)\n",
    "        \"\"\"\n",
    "        self.deltas = []\n",
    "    \n",
    "    def _forward_prop(self, x):\n",
    "        # set the first activation to be the data point itself\n",
    "        self.activations.append(x)\n",
    "        # update the rest of the activations\n",
    "        for l in range(len(self.weights)):\n",
    "            # Get the pre-combination of the next layer\n",
    "            z_plus1 = np.dot(self.weights[l], self.activations[l])\n",
    "            \"\"\"\n",
    "            Apply activation.\n",
    "            If in the output layer, use softmax.\n",
    "            Else, use sigmoid, and add a bias.\n",
    "            \"\"\"\n",
    "            if l == len(self.weights) - 1:\n",
    "                a_plus1 = softmax(z_plus1)\n",
    "            else:\n",
    "                a_plus1 = np.append(1,sigmoid(z_plus1)) # append the bias\n",
    "            # add to list of activations\n",
    "            self.activations.append(a_plus1)\n",
    "    \n",
    "    def _back_prop(self, y):\n",
    "        \n",
    "        # compute the prediction error - difference between prediction and truth\n",
    "        e = self.activations[-1] - y # vector subtraction\n",
    "        self.deltas.append(e)\n",
    "        # compute the rest of the errors\n",
    "        for l in reversed(range(1,len(self.weights))):\n",
    "            # get the next error\n",
    "            delta_plus1 = self.deltas[0]\n",
    "            # get the current weight and activatios\n",
    "            W_l = self.weights[l]\n",
    "            a_l = self.activations[l]\n",
    "            # compute the current derivative with respect to the activation (sigmoid)\n",
    "            g_l = sigmoid_deriv(a_l)\n",
    "            # compute the current error\n",
    "            delta_l = np.multiply(np.dot(W_l.T, delta_plus1),g_l)\n",
    "            # add the error to the front of the list\n",
    "            self.deltas.insert(0, delta_l)\n",
    "        \"\"\"\n",
    "        Update the partial derivatives of the weights\n",
    "        \"\"\"\n",
    "        for k in range(len(self.weights)):\n",
    "            if k > 0:\n",
    "                self.derivatives[k] += np.outer(self.deltas[k], self.activations[k].T)\n",
    "            else:\n",
    "                self.derivatives[k] += np.outer(self.deltas[k][1:], self.activations[k].T)\n",
    "    \n",
    "    def train(self,X, y, eta = .001, epochs = 10, print_every = 10):\n",
    "        \"\"\"\n",
    "        response y is expected to be in one-hot encoding. \n",
    "        convert it to be as such\n",
    "        \"\"\"\n",
    "        y_onehot = np.zeros((X.shape[0], self.layers_dim[-1]))\n",
    "        for i in range(len(y)):\n",
    "            tmp = np.zeros(self.layers_dim[-1])\n",
    "            tmp[int(y[i])] = 1\n",
    "            y_onehot[i] = tmp\n",
    "        \n",
    "        \"\"\"\n",
    "        Store the number of data points\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        \"\"\"\n",
    "        perform stochastic gradient descent\n",
    "        \"\"\"\n",
    "        for e in range(epochs):\n",
    "            # reset the predictions from previous epoch\n",
    "            self._reset_predictions()\n",
    "            if e % print_every == 0:\n",
    "                print(\"Epoch %d: MSE = %f\" %(e, self.errors[-1]))\n",
    "            \"\"\"\n",
    "            SGD: Update for every training example\n",
    "            \"\"\"\n",
    "            for i in range(X.shape[0]):\n",
    "                # current example and respunse\n",
    "                x, _y = X[i], y_onehot[i]\n",
    "                \n",
    "                # reset previous acivations and errors\n",
    "                self._reset_activations()\n",
    "                self._reset_deltas()\n",
    "                self._reset_derivatives()\n",
    "                # forward propogate\n",
    "                self._forward_prop(x)\n",
    "                # backwards propogate\n",
    "                self._back_prop(_y)\n",
    "                # update the weights using the derivatives\n",
    "                for l in range(len(self.weights)):\n",
    "                    self.weights[l] -= eta*self.derivatives[l]\n",
    "                \n",
    "                # add the prediction of that example to the list\n",
    "                self.predictions.append(np.argmax(self.activations[-1]))\n",
    "            \n",
    "            \"\"\"\n",
    "            End of epoch. \n",
    "            Calculate error, and store it.\n",
    "            \"\"\"\n",
    "            # current prediction\n",
    "            pred = self.predictions\n",
    "            self.errors.append(mse(pred, y))\n",
    "                    \n",
    "    def _reset_activations(self):\n",
    "        self.activations = []\n",
    "    \n",
    "    def _reset_deltas(self):\n",
    "        self.deltas = []\n",
    "    \n",
    "    def _reset_predictions(self):\n",
    "        self.predictions = []\n",
    "    \n",
    "    def _reset_derivatives(self):\n",
    "        self.derivatives = []\n",
    "        for weight in self.weights:\n",
    "            self.derivatives.append(np.zeros(weight.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training - After circle preprocessing\n",
    "\n",
    "Recall - there are four pre-processing schemes. Based on the logistic regression results, the dataset with the \"circle heuristic\" works best. I'll only use this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"../data/preproccessed/circle/X_trainnorm.npy\")\n",
    "y = np.load(\"../data/preproccessed/circle/y_trainnorm.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 28, 28), (50000, 1))"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have to unroll the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(50000, 28*28)\n",
    "y = y.reshape(50000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A first architecture\n",
    "\n",
    "Here, we'll have one hidden layer of 64 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1 = NNet(input_dim = 28*28, layers_dim=(64,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: MSE = 693339.000000\n"
     ]
    }
   ],
   "source": [
    "net1.train(X_train, y_train, print_every=1, eta = .1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

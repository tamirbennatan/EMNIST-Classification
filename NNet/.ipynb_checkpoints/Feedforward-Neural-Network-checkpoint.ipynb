{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "import math\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. A neural network class (feedforward, fully connected)\n",
    "\n",
    "Architectures are configurable. However, it only supports Stochastic Gradient Descent training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_deriv(x):\n",
    "    return x*(1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_pred, y_true):\n",
    "    return np.sum((y_pred - y_true)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet(object):\n",
    "    \n",
    "    def __init__(self, input_dim, layers_dim):\n",
    "        # store problem metadata\n",
    "        self.layers_dim = layers_dim\n",
    "        self.input_dim = input_dim\n",
    "        # keep track of errors - to make sure they're goig down\n",
    "        self.errors = [math.inf]\n",
    "        self.predictions = []\n",
    "        \"\"\"\n",
    "        Initialize weights\n",
    "        \"\"\"\n",
    "        self.weights = []\n",
    "        # get number of neurons in first hidden layer\n",
    "        k = layers_dim[0]\n",
    "        # initialize first weight matrix\n",
    "        W1 = np.random.random((k, input_dim))\n",
    "        self.weights.append(W1)\n",
    "        # Add the rest of the dimensions\n",
    "        for i in range(len(layers_dim) - 1):\n",
    "            # previous and post dimension\n",
    "            prev_dim, next_dim = layers_dim[i], layers_dim[i+1]\n",
    "            self.weights.append(np.random.random((next_dim, prev_dim + 1)))\n",
    "        \n",
    "        \"\"\"\n",
    "        Keep track of the partial derivatives with regards to each weight. \n",
    "        We will build them up using batch gradient descent\n",
    "        \"\"\"\n",
    "        self.derivatives = []\n",
    "        for weight in self.weights:\n",
    "            self.derivatives.append(np.zeros(weight.shape))\n",
    "        \n",
    "        \"\"\"\n",
    "        Keep track of the activations. \n",
    "        \"\"\"\n",
    "        self.activations = []\n",
    "        \"\"\"\n",
    "        Keep track of the errors (delta)\n",
    "        \"\"\"\n",
    "        self.deltas = []\n",
    "    \n",
    "    def _forward_prop(self, x):\n",
    "        # set the first activation to be the data point itself\n",
    "        self.activations.append(x)\n",
    "        # update the rest of the activations\n",
    "        for l in range(len(self.weights)):\n",
    "            # Get the pre-combination of the next layer\n",
    "            z_plus1 = np.dot(self.weights[l], self.activations[l])\n",
    "            \"\"\"\n",
    "            Apply activation.\n",
    "            If in the output layer, use softmax.\n",
    "            Else, use sigmoid, and add a bias.\n",
    "            \"\"\"\n",
    "            if l == len(self.weights) - 1:\n",
    "                a_plus1 = softmax(z_plus1)\n",
    "            else:\n",
    "                a_plus1 = np.append(1,sigmoid(z_plus1)) # append the bias\n",
    "            # add to list of activations\n",
    "            self.activations.append(a_plus1)\n",
    "    \n",
    "    def _back_prop(self, y):\n",
    "        \n",
    "        # compute the prediction error - difference between prediction and truth\n",
    "        e = self.activations[-1] - y # vector subtraction\n",
    "        self.deltas.append(e)\n",
    "        # compute the rest of the errors\n",
    "        for l in reversed(range(1,len(self.weights))):\n",
    "            # get the next error\n",
    "            delta_plus1 = self.deltas[0]\n",
    "            # get the current weight and activatios\n",
    "            W_l = self.weights[l]\n",
    "            a_l = self.activations[l]\n",
    "            # compute the current derivative with respect to the activation (sigmoid)\n",
    "            g_l = sigmoid_deriv(a_l)\n",
    "            # compute the current error\n",
    "            delta_l = np.multiply(np.dot(W_l.T, delta_plus1),g_l)\n",
    "            # add the error to the front of the list\n",
    "            self.deltas.insert(0, delta_l)\n",
    "        \"\"\"\n",
    "        Update the partial derivatives of the weights\n",
    "        \"\"\"\n",
    "        for k in range(len(self.weights)):\n",
    "            if k > 0:\n",
    "                self.derivatives[k] += np.outer(self.deltas[k], self.activations[k].T)\n",
    "            else:\n",
    "                self.derivatives[k] += np.outer(self.deltas[k][1:], self.activations[k].T)\n",
    "    \n",
    "    def train(self,X, y, eta = .001, epochs = 10, print_every = 10):\n",
    "        \"\"\"\n",
    "        response y is expected to be in one-hot encoding. \n",
    "        convert it to be as such\n",
    "        \"\"\"\n",
    "        y_onehot = np.zeros((X.shape[0], self.layers_dim[-1]))\n",
    "        for i in range(len(y)):\n",
    "            tmp = np.zeros(self.layers_dim[-1])\n",
    "            tmp[int(y[i])] = 1\n",
    "            y_onehot[i] = tmp\n",
    "        \n",
    "        \"\"\"\n",
    "        Store the number of data points\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        \"\"\"\n",
    "        perform stochastic gradient descent\n",
    "        \"\"\"\n",
    "        for e in range(epochs):\n",
    "            # reset the predictions from previous epoch\n",
    "            self._reset_predictions()\n",
    "            if e % print_every == 0:\n",
    "                print(\"Epoch %d: MSE = %f\" %(e, self.errors[-1]))\n",
    "            \"\"\"\n",
    "            SGD: Update for every training example\n",
    "            \"\"\"\n",
    "            for i in range(X.shape[0]):\n",
    "                # current example and respunse\n",
    "                x, _y = X[i], y_onehot[i]\n",
    "                \n",
    "                # reset previous acivations and errors\n",
    "                self._reset_activations()\n",
    "                self._reset_deltas()\n",
    "                self._reset_derivatives()\n",
    "                # forward propogate\n",
    "                self._forward_prop(x)\n",
    "                # backwards propogate\n",
    "                self._back_prop(_y)\n",
    "                # update the weights using the derivatives\n",
    "                for l in range(len(self.weights)):\n",
    "                    self.weights[l] -= eta*self.derivatives[l]\n",
    "                \n",
    "                # add the prediction of that example to the list\n",
    "                self.predictions.append(np.argmax(self.activations[-1]))\n",
    "            \n",
    "            \"\"\"\n",
    "            End of epoch. \n",
    "            Calculate error, and store it.\n",
    "            \"\"\"\n",
    "            # current prediction\n",
    "            pred = self.predictions\n",
    "            self.errors.append(mse(pred, y))\n",
    "                    \n",
    "    def _reset_activations(self):\n",
    "        self.activations = []\n",
    "    \n",
    "    def _reset_deltas(self):\n",
    "        self.deltas = []\n",
    "    \n",
    "    def _reset_predictions(self):\n",
    "        self.predictions = []\n",
    "    \n",
    "    def _reset_derivatives(self):\n",
    "        self.derivatives = []\n",
    "        for weight in self.weights:\n",
    "            self.derivatives.append(np.zeros(weight.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data - Circle preprocessing\n",
    "\n",
    "Recall - there are four pre-processing schemes. Based on the logistic regression results, the dataset with the \"circle heuristic\" works best. I'll only use this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"../data/preproccessed/circle/X_trainnorm.npy\")\n",
    "y = np.load(\"../data/preproccessed/circle/y_trainnorm.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 28, 28), (50000, 1))"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have to unroll the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(50000, 28*28)\n",
    "y = y.reshape(50000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A dummy example - Model validation\n",
    "\n",
    "Just to see that the network is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many classes? \n",
    "len(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NNet(input_dim = 2, layers_dim = (5, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: MSE = inf\n",
      "Epoch 100: MSE = 190.000000\n",
      "Epoch 200: MSE = 59.000000\n",
      "Epoch 300: MSE = 48.000000\n",
      "Epoch 400: MSE = 43.000000\n",
      "Epoch 500: MSE = 42.000000\n",
      "Epoch 600: MSE = 42.000000\n",
      "Epoch 700: MSE = 44.000000\n",
      "Epoch 800: MSE = 43.000000\n",
      "Epoch 900: MSE = 43.000000\n",
      "Epoch 1000: MSE = 43.000000\n",
      "Epoch 1100: MSE = 43.000000\n",
      "Epoch 1200: MSE = 43.000000\n",
      "Epoch 1300: MSE = 43.000000\n",
      "Epoch 1400: MSE = 42.000000\n",
      "Epoch 1500: MSE = 42.000000\n",
      "Epoch 1600: MSE = 42.000000\n",
      "Epoch 1700: MSE = 42.000000\n",
      "Epoch 1800: MSE = 42.000000\n",
      "Epoch 1900: MSE = 42.000000\n",
      "Epoch 2000: MSE = 42.000000\n",
      "Epoch 2100: MSE = 42.000000\n",
      "Epoch 2200: MSE = 42.000000\n",
      "Epoch 2300: MSE = 42.000000\n",
      "Epoch 2400: MSE = 42.000000\n",
      "Epoch 2500: MSE = 41.000000\n",
      "Epoch 2600: MSE = 41.000000\n",
      "Epoch 2700: MSE = 40.000000\n",
      "Epoch 2800: MSE = 40.000000\n",
      "Epoch 2900: MSE = 40.000000\n",
      "Epoch 3000: MSE = 40.000000\n",
      "Epoch 3100: MSE = 40.000000\n",
      "Epoch 3200: MSE = 40.000000\n",
      "Epoch 3300: MSE = 41.000000\n",
      "Epoch 3400: MSE = 41.000000\n",
      "Epoch 3500: MSE = 41.000000\n",
      "Epoch 3600: MSE = 41.000000\n",
      "Epoch 3700: MSE = 41.000000\n",
      "Epoch 3800: MSE = 41.000000\n",
      "Epoch 3900: MSE = 40.000000\n",
      "Epoch 4000: MSE = 40.000000\n",
      "Epoch 4100: MSE = 40.000000\n",
      "Epoch 4200: MSE = 39.000000\n",
      "Epoch 4300: MSE = 39.000000\n",
      "Epoch 4400: MSE = 39.000000\n",
      "Epoch 4500: MSE = 39.000000\n",
      "Epoch 4600: MSE = 40.000000\n",
      "Epoch 4700: MSE = 40.000000\n",
      "Epoch 4800: MSE = 40.000000\n",
      "Epoch 4900: MSE = 40.000000\n",
      "Epoch 5000: MSE = 40.000000\n",
      "Epoch 5100: MSE = 40.000000\n",
      "Epoch 5200: MSE = 39.000000\n",
      "Epoch 5300: MSE = 39.000000\n",
      "Epoch 5400: MSE = 39.000000\n",
      "Epoch 5500: MSE = 39.000000\n",
      "Epoch 5600: MSE = 39.000000\n",
      "Epoch 5700: MSE = 39.000000\n",
      "Epoch 5800: MSE = 39.000000\n",
      "Epoch 5900: MSE = 39.000000\n",
      "Epoch 6000: MSE = 39.000000\n",
      "Epoch 6100: MSE = 39.000000\n",
      "Epoch 6200: MSE = 39.000000\n",
      "Epoch 6300: MSE = 39.000000\n",
      "Epoch 6400: MSE = 40.000000\n",
      "Epoch 6500: MSE = 40.000000\n",
      "Epoch 6600: MSE = 39.000000\n",
      "Epoch 6700: MSE = 39.000000\n",
      "Epoch 6800: MSE = 39.000000\n",
      "Epoch 6900: MSE = 39.000000\n",
      "Epoch 7000: MSE = 39.000000\n",
      "Epoch 7100: MSE = 39.000000\n",
      "Epoch 7200: MSE = 39.000000\n",
      "Epoch 7300: MSE = 39.000000\n",
      "Epoch 7400: MSE = 38.000000\n",
      "Epoch 7500: MSE = 38.000000\n",
      "Epoch 7600: MSE = 38.000000\n",
      "Epoch 7700: MSE = 38.000000\n",
      "Epoch 7800: MSE = 38.000000\n",
      "Epoch 7900: MSE = 38.000000\n",
      "Epoch 8000: MSE = 38.000000\n",
      "Epoch 8100: MSE = 38.000000\n",
      "Epoch 8200: MSE = 38.000000\n",
      "Epoch 8300: MSE = 38.000000\n",
      "Epoch 8400: MSE = 37.000000\n",
      "Epoch 8500: MSE = 37.000000\n",
      "Epoch 8600: MSE = 37.000000\n",
      "Epoch 8700: MSE = 37.000000\n",
      "Epoch 8800: MSE = 37.000000\n",
      "Epoch 8900: MSE = 37.000000\n",
      "Epoch 9000: MSE = 37.000000\n",
      "Epoch 9100: MSE = 37.000000\n",
      "Epoch 9200: MSE = 37.000000\n",
      "Epoch 9300: MSE = 36.000000\n",
      "Epoch 9400: MSE = 36.000000\n",
      "Epoch 9500: MSE = 36.000000\n",
      "Epoch 9600: MSE = 35.000000\n",
      "Epoch 9700: MSE = 36.000000\n",
      "Epoch 9800: MSE = 36.000000\n",
      "Epoch 9900: MSE = 35.000000\n"
     ]
    }
   ],
   "source": [
    "net.train(X,y, epochs= 10000, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  0,  0],\n",
       "       [ 1, 29, 13],\n",
       "       [ 0, 21, 37]])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(net.predictions, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so its working!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A first architecture\n",
    "\n",
    "Here, we'll have one hidden layer of 64 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1 = NNet(input_dim = 28*28, layers_dim=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: MSE = inf\n",
      "Epoch 1: MSE = 707207.000000\n",
      "Epoch 2: MSE = 701527.000000\n",
      "Epoch 3: MSE = 701527.000000\n",
      "Epoch 4: MSE = 701527.000000\n",
      "Epoch 5: MSE = 701527.000000\n",
      "Epoch 6: MSE = 701527.000000\n",
      "Epoch 7: MSE = 701527.000000\n",
      "Epoch 8: MSE = 701527.000000\n",
      "Epoch 9: MSE = 701527.000000\n"
     ]
    }
   ],
   "source": [
    "net1.train(X_train, y_train, print_every=1, eta = .001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training takes way too long.. going to move to AWS to train on their machines so that I can at least use their processors. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

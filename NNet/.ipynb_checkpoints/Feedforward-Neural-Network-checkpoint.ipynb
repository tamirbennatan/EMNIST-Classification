{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "import math\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. A neural network class (feedforward, fully connected)\n",
    "\n",
    "Architectures are configurable. However, it only supports Stochastic Gradient Descent training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_deriv(x):\n",
    "    return x*(1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_pred, y_true):\n",
    "    return np.sum((y_pred - y_true)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet(object):\n",
    "    \n",
    "    def __init__(self, input_dim, layers_dim):\n",
    "        # store problem metadata\n",
    "        self.layers_dim = layers_dim\n",
    "        self.input_dim = input_dim\n",
    "        # keep track of errors - to make sure they're goig down\n",
    "        self.errors = [math.inf]\n",
    "        self.predictions = []\n",
    "        \"\"\"\n",
    "        Initialize weights\n",
    "        \"\"\"\n",
    "        self.weights = []\n",
    "        # get number of neurons in first hidden layer\n",
    "        k = layers_dim[0]\n",
    "        # initialize first weight matrix\n",
    "        W1 = np.random.random((k, input_dim))\n",
    "        self.weights.append(W1)\n",
    "        # Add the rest of the dimensions\n",
    "        for i in range(len(layers_dim) - 1):\n",
    "            # previous and post dimension\n",
    "            prev_dim, next_dim = layers_dim[i], layers_dim[i+1]\n",
    "            self.weights.append(np.random.random((next_dim, prev_dim)))\n",
    "        \n",
    "        \"\"\"\n",
    "        initialize the biases\n",
    "        Only the hidden layers get biases\n",
    "        \"\"\"\n",
    "        self.biases = {}\n",
    "        hidden_dimensions = layers_dim[:-1]\n",
    "        for l in range(len(hidden_dimensions)):\n",
    "            self.biases[l + 1] = np.random.random(hidden_dimensions[l])\n",
    "        \"\"\"\n",
    "        Keep track of the partial derivatives with regards to each weight. \n",
    "        We will build them up using batch gradient descent\n",
    "        \"\"\"\n",
    "        self.derivatives = []\n",
    "        for weight in self.weights:\n",
    "            self.derivatives.append(np.zeros(weight.shape))\n",
    "        \n",
    "        \"\"\"\n",
    "        Keep track of the activations. \n",
    "        \"\"\"\n",
    "        self.activations = []\n",
    "        \"\"\"\n",
    "        Keep track of the errors (delta)\n",
    "        \"\"\"\n",
    "        self.deltas = []\n",
    "    \n",
    "    def _forward_prop(self, x):\n",
    "        # set the first activation to be the data point itself\n",
    "        self.activations.append(x)\n",
    "        # update the rest of the activations\n",
    "        for l in range(len(self.weights)):\n",
    "            # Get the pre-combination of the next layer\n",
    "            z_plus1 = np.dot(self.weights[l], self.activations[l])\n",
    "            \"\"\"\n",
    "            Apply activation.\n",
    "            If the layer is a hidden layer, add the biases. \n",
    "            \"\"\"\n",
    "            if l+1 in self.biases:\n",
    "                a_plus1 = sigmoid(z_plus1 + self.biases[l+1])\n",
    "            else:\n",
    "                a_plus1 = sigmoid(z_plus1)\n",
    "            # add to list of activations\n",
    "            self.activations.append(a_plus1)\n",
    "    \n",
    "    def _back_prop(self, y):\n",
    "        \n",
    "        # compute the prediction error - difference between prediction and truth\n",
    "        e = self.activations[-1] - y # vector subtraction\n",
    "        self.deltas = [e]\n",
    "        # compute the rest of the errors\n",
    "        for l in reversed(range(1,len(self.weights))):\n",
    "            # get the next error\n",
    "            delta_plus1 = self.deltas[0]\n",
    "            # get the current weight and activatios\n",
    "            W_l = self.weights[l]\n",
    "            a_l = self.activations[l]\n",
    "            # compute the current derivative with respect to the activation (sigmoid)\n",
    "            g_l = sigmoid_deriv(a_l)\n",
    "            # compute the current error\n",
    "            delta_l = np.multiply(np.dot(W_l.T, delta_plus1),g_l)\n",
    "            # add the error to the front of the list\n",
    "            self.deltas.insert(0, delta_l)\n",
    "        \"\"\"\n",
    "        Update the partial derivatives of the weights\n",
    "        \"\"\"\n",
    "        for k in range(len(self.weights)):\n",
    "            self.derivatives[k] += np.outer(self.deltas[k], self.activations[k].T)\n",
    "    \n",
    "    def train(self,X, y, eta = .001, epochs = 10, print_every = 10):\n",
    "        \"\"\"\n",
    "        response y is expected to be in one-hot encoding. \n",
    "        convert it to be as such\n",
    "        \"\"\"\n",
    "        y_onehot = np.zeros((X.shape[0], self.layers_dim[-1]))\n",
    "        for i in range(len(y)):\n",
    "            tmp = np.zeros(self.layers_dim[-1])\n",
    "            tmp[int(y[i])] = 1\n",
    "            y_onehot[i] = tmp\n",
    "        \n",
    "        \"\"\"\n",
    "        Store the number of data points\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        \"\"\"\n",
    "        perform stochastic gradient descent\n",
    "        \"\"\"\n",
    "        for e in range(epochs):\n",
    "            # reset the predictions from previous epoch\n",
    "            self._reset_predictions()\n",
    "            if e % print_every == 0:\n",
    "                print(\"Epoch %d: MSE = %f\" %(e, self.errors[-1]))\n",
    "            \"\"\"\n",
    "            SGD: Update for every training example\n",
    "            \"\"\"\n",
    "            for i in range(X.shape[0]):\n",
    "                # current example and respunse\n",
    "                x, _y = X[i], y_onehot[i]\n",
    "                \n",
    "                # reset previous acivations and errors\n",
    "                self._reset_activations()\n",
    "                self._reset_deltas()\n",
    "                self._reset_derivatives()\n",
    "                # forward propogate\n",
    "                self._forward_prop(x)\n",
    "                # backwards propogate\n",
    "                self._back_prop(_y)\n",
    "                # update the weights using the derivatives\n",
    "                for l in range(len(self.weights)):\n",
    "                    self.weights[l] -= eta*self.derivatives[l]\n",
    "                # update the biases using the errors\n",
    "                for k in self.biases:\n",
    "                    self.biases[k] -= eta*self.deltas[k - 1]\n",
    "                \n",
    "                # add the prediction of that example to the list\n",
    "                self.predictions.append(np.argmax(self.activations[-1]))\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            End of epoch. \n",
    "            Calculate error, and store it.\n",
    "            \"\"\"\n",
    "            # current prediction\n",
    "            pred = self.predictions\n",
    "            self.errors.append(mse(pred, y))\n",
    "    \n",
    "    def predict(self, X_new):\n",
    "        # store the predictions in a list for now\n",
    "        predictions = []\n",
    "        # for each of the training examples, forward propogate\n",
    "        for x in X_new:\n",
    "            self._reset_activations()\n",
    "            self._forward_prop(x)\n",
    "            # add the prediction to the predictions list\n",
    "            predictions.append(np.argmax(self.activations[-1]))\n",
    "            \n",
    "        return(np.array(predictions))\n",
    "        \n",
    "                    \n",
    "    def _reset_activations(self):\n",
    "        self.activations = []\n",
    "    \n",
    "    def _reset_deltas(self):\n",
    "        self.deltas = []\n",
    "    \n",
    "    def _reset_predictions(self):\n",
    "        self.predictions = []\n",
    "    \n",
    "    def _reset_derivatives(self):\n",
    "        self.derivatives = []\n",
    "        for weight in self.weights:\n",
    "            self.derivatives.append(np.zeros(weight.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A dummy example - model validation\n",
    "\n",
    "I'll apply the network on the classic Iris dataset to verify that it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num classes in y\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NNet(input_dim = 4, layers_dim = (20, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: MSE = inf\n",
      "Epoch 100: MSE = 47.000000\n",
      "Epoch 200: MSE = 43.000000\n",
      "Epoch 300: MSE = 25.000000\n",
      "Epoch 400: MSE = 14.000000\n",
      "Epoch 500: MSE = 8.000000\n",
      "Epoch 600: MSE = 8.000000\n",
      "Epoch 700: MSE = 5.000000\n",
      "Epoch 800: MSE = 4.000000\n",
      "Epoch 900: MSE = 4.000000\n"
     ]
    }
   ],
   "source": [
    "net.train(X,y, epochs= 1000, print_every=100, eta = .001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(net.predict(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IT WORKS :) :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data - Circle preprocessing\n",
    "\n",
    "Recall - there are four pre-processing schemes. Based on the logistic regression results, the dataset with the \"circle heuristic\" works best. I'll only use this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"../data/preproccessed/circle/X_trainnorm.npy\")\n",
    "y = np.load(\"../data/preproccessed/circle/y_trainnorm.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 28, 28), (50000, 1))"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have to unroll the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(50000, 28*28)\n",
    "y = y.reshape(50000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A first architecture\n",
    "\n",
    "Here, we'll have one hidden layer of 64 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1 = NNet(input_dim = 28*28, layers_dim=(100,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: MSE = inf\n",
      "Epoch 1: MSE = 664180.000000\n",
      "Epoch 2: MSE = 664326.000000\n",
      "Epoch 3: MSE = 664326.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-652-897ea5921189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-607-941f5d9c0631>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, eta, epochs, print_every)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;31m# update the weights using the derivatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mderivatives\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# update the biases using the errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net1.train(X_train, y_train, print_every=1, eta = .01, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too slow... I'll train on AWS so that I can at least turn of my computer.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

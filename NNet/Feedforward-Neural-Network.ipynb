{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "import math\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. A neural network class (feedforward, fully connected)\n",
    "\n",
    "Architectures are configurable. However, it only supports Stochastic Gradient Descent training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_deriv(x):\n",
    "    return x*(1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_pred, y_true):\n",
    "    return np.sum((y_pred - y_true)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet(object):\n",
    "    \n",
    "    def __init__(self, input_dim, layers_dim, gamma = .9):\n",
    "        # store classifier metadata\n",
    "        self.layers_dim = layers_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.gamma = gamma\n",
    "        # keep track of errors - to make sure they're goig down\n",
    "        self.errors = [math.inf]\n",
    "        self.predictions = []\n",
    "        \"\"\"\n",
    "        Initialize weights\n",
    "        \"\"\"\n",
    "        self.weights = []\n",
    "        # get number of neurons in first hidden layer\n",
    "        k = layers_dim[0]\n",
    "        # initialize first weight matrix\n",
    "        W1 = np.random.random((k, input_dim))\n",
    "        self.weights.append(W1)\n",
    "        # Add the rest of the dimensions\n",
    "        for i in range(len(layers_dim) - 1):\n",
    "            # previous and post dimension\n",
    "            prev_dim, next_dim = layers_dim[i], layers_dim[i+1]\n",
    "            self.weights.append(np.random.random((next_dim, prev_dim)))\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        self.weight_change = []\n",
    "        for W in self.weights:\n",
    "            self.weight_change.append(np.zeros(W.shape))\n",
    "        \n",
    "        \"\"\"\n",
    "        initialize the biases\n",
    "        Only the hidden layers get biases\n",
    "        \"\"\"\n",
    "        self.biases = {}\n",
    "        hidden_dimensions = layers_dim[:-1]\n",
    "        for l in range(len(hidden_dimensions)):\n",
    "            self.biases[l + 1] = np.random.random(hidden_dimensions[l])\n",
    "        \"\"\"\n",
    "        Keep track of the partial derivatives with regards to each weight. \n",
    "        We will build them up using batch gradient descent\n",
    "        \"\"\"\n",
    "        self.derivatives = []\n",
    "        for weight in self.weights:\n",
    "            self.derivatives.append(np.zeros(weight.shape))\n",
    "        \n",
    "        \"\"\"\n",
    "        Keep track of the activations. \n",
    "        \"\"\"\n",
    "        self.activations = []\n",
    "        \"\"\"\n",
    "        Keep track of the errors (delta)\n",
    "        \"\"\"\n",
    "        self.deltas = []\n",
    "    \n",
    "    def _forward_prop(self, x):\n",
    "        # set the first activation to be the data point itself\n",
    "        self.activations.append(x)\n",
    "        # update the rest of the activations\n",
    "        for l in range(len(self.weights)):\n",
    "            # Get the pre-combination of the next layer\n",
    "            z_plus1 = np.dot(self.weights[l], self.activations[l])\n",
    "            \"\"\"\n",
    "            Apply activation.\n",
    "            If the layer is a hidden layer, add the biases. \n",
    "            \"\"\"\n",
    "            if l+1 in self.biases:\n",
    "                a_plus1 = sigmoid(z_plus1 + self.biases[l+1])\n",
    "            else:\n",
    "                a_plus1 = sigmoid(z_plus1)\n",
    "            # add to list of activations\n",
    "            self.activations.append(a_plus1)\n",
    "    \n",
    "    def _back_prop(self, y):\n",
    "        \n",
    "        # compute the prediction error - difference between prediction and truth\n",
    "        e = self.activations[-1] - y # vector subtraction\n",
    "        self.deltas = [e]\n",
    "        # compute the rest of the errors\n",
    "        for l in reversed(range(1,len(self.weights))):\n",
    "            # get the next error\n",
    "            delta_plus1 = self.deltas[0]\n",
    "            # get the current weight and activatios\n",
    "            W_l = self.weights[l]\n",
    "            a_l = self.activations[l]\n",
    "            # compute the current derivative with respect to the activation (sigmoid)\n",
    "            g_l = sigmoid_deriv(a_l)\n",
    "            # compute the current error\n",
    "            delta_l = np.multiply(np.dot(W_l.T, delta_plus1),g_l)\n",
    "            # add the error to the front of the list\n",
    "            self.deltas.insert(0, delta_l)\n",
    "        \"\"\"\n",
    "        Update the partial derivatives of the weights\n",
    "        \"\"\"\n",
    "        for k in range(len(self.weights)):\n",
    "            self.derivatives[k] += np.outer(self.deltas[k], self.activations[k].T)\n",
    "    \n",
    "    def train(self,X, y, eta = .001, epochs = 10, print_every = 10, scaling = 1.0):\n",
    "        \"\"\"\n",
    "        response y is expected to be in one-hot encoding. \n",
    "        convert it to be as such\n",
    "        \"\"\"\n",
    "        y_onehot = np.zeros((X.shape[0], self.layers_dim[-1]))\n",
    "        for i in range(len(y)):\n",
    "            tmp = np.zeros(self.layers_dim[-1])\n",
    "            tmp[int(y[i])] = 1\n",
    "            y_onehot[i] = tmp\n",
    "        \n",
    "        \"\"\"\n",
    "        Store the number of data points\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        \"\"\"\n",
    "        perform stochastic gradient descent\n",
    "        \"\"\"\n",
    "        for e in range(epochs):\n",
    "            # reset the predictions from previous epoch\n",
    "            self._reset_predictions()\n",
    "            if e % print_every == 0:\n",
    "                print(\"Epoch %d: MSE = %f\" %(e, self.errors[-1]))\n",
    "            \"\"\"\n",
    "            SGD: Update for every training example\n",
    "            \"\"\"\n",
    "            for i in range(X.shape[0]):\n",
    "                # current example and respunse\n",
    "                x, _y = X[i], y_onehot[i]\n",
    "                \n",
    "                # reset previous acivations and errors\n",
    "                self._reset_activations()\n",
    "                self._reset_deltas()\n",
    "                self._reset_derivatives()\n",
    "                # forward propogate\n",
    "                self._forward_prop(x)\n",
    "                # backwards propogate\n",
    "                self._back_prop(_y)\n",
    "                # update the weights using the derivatives\n",
    "                for l in range(len(self.weights)):\n",
    "                    if gamma < 1:\n",
    "                        # Calculate the change in the weights that needs to be made\n",
    "                        weight_change = self.gamma*self.weight_change[l] + eta*self.derivatives[l]\n",
    "                        # update the weights\n",
    "                        self.weights[l] -= weight_change\n",
    "                        # store the weight change for next time\n",
    "                        self.weight_change[l] = weight_change\n",
    "                    else:\n",
    "                        self.weights[l] -= eta*self.derivatives[l]\n",
    "                    \n",
    "                # update the biases using the errors\n",
    "                for k in self.biases:\n",
    "                    self.biases[k] -= eta*self.deltas[k - 1]\n",
    "                \n",
    "                # add the prediction of that example to the list\n",
    "                self.predictions.append(np.argmax(self.activations[-1]))\n",
    "            \n",
    "            \"\"\"\n",
    "            End of epoch. \n",
    "            Calculate error, and store it.\n",
    "            \n",
    "            Also rescale learning rate\n",
    "            \"\"\"\n",
    "            # current prediction\n",
    "            pred = self.predictions\n",
    "            self.errors.append(mse(pred, y))\n",
    "            # rescale learning rate\n",
    "            eta = eta/(np.power(e + 1,scaling))\n",
    "    \n",
    "    def predict(self, X_new):\n",
    "        # store the predictions in a list for now\n",
    "        predictions = []\n",
    "        # for each of the training examples, forward propogate\n",
    "        for x in X_new:\n",
    "            self._reset_activations()\n",
    "            self._forward_prop(x)\n",
    "            # add the prediction to the predictions list\n",
    "            predictions.append(np.argmax(self.activations[-1]))\n",
    "            \n",
    "        return(np.array(predictions))\n",
    "        \n",
    "                    \n",
    "    def _reset_activations(self):\n",
    "        self.activations = []\n",
    "    \n",
    "    def _reset_deltas(self):\n",
    "        self.deltas = []\n",
    "    \n",
    "    def _reset_predictions(self):\n",
    "        self.predictions = []\n",
    "    \n",
    "    def _reset_derivatives(self):\n",
    "        self.derivatives = []\n",
    "        for weight in self.weights:\n",
    "            self.derivatives.append(np.zeros(weight.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A dummy example - model validation\n",
    "\n",
    "I'll apply the network on the classic Iris dataset to verify that it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num classes in y\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NNet(input_dim = 4, layers_dim = (20, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: MSE = inf\n",
      "Epoch 100: MSE = 11.000000\n",
      "Epoch 200: MSE = 6.000000\n",
      "Epoch 300: MSE = 9.000000\n",
      "Epoch 400: MSE = 11.000000\n",
      "Epoch 500: MSE = 11.000000\n",
      "Epoch 600: MSE = 8.000000\n",
      "Epoch 700: MSE = 9.000000\n",
      "Epoch 800: MSE = 8.000000\n",
      "Epoch 900: MSE = 7.000000\n"
     ]
    }
   ],
   "source": [
    "net.train(X,y, epochs= 1000, print_every=100, eta = .001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(net.predict(X), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IT WORKS :) :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data - Circle preprocessing\n",
    "\n",
    "Recall - there are four pre-processing schemes. Based on the logistic regression results, the dataset with the \"circle heuristic\" works best. I'll only use this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"../data/preproccessed/circle/X_trainnorm.npy\")\n",
    "y = np.load(\"../data/preproccessed/circle/y_trainnorm.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 28, 28), (50000, 1))"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have to unroll the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(50000, 28*28)\n",
    "y = y.reshape(50000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. A first architecture\n",
    "\n",
    "Here, we'll have one hidden layer of 64 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1 = NNet(input_dim = 28*28, layers_dim=(128,10), gamma = .99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: MSE = inf\n",
      "Epoch 1: MSE = 655114.000000\n",
      "Epoch 2: MSE = 663528.000000\n",
      "Epoch 3: MSE = 662018.000000\n",
      "Epoch 4: MSE = 659532.000000\n",
      "Epoch 5: MSE = 666435.000000\n",
      "Epoch 6: MSE = 665328.000000\n",
      "Epoch 7: MSE = 668678.000000\n",
      "Epoch 8: MSE = 664328.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-692-1c8f37971baf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-679-e32ca8ef3e60>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, eta, epochs, print_every, scaling)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# backwards propogate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_back_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m                 \u001b[0;31m# update the weights using the derivatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-679-e32ca8ef3e60>\u001b[0m in \u001b[0;36m_back_prop\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \"\"\"\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mderivatives\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mouter\u001b[0;34m(a, b, out)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net1.train(X_train, y_train, print_every=1, eta = .01, epochs=100, scaling=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too slow... I'll train on AWS so that I can at least turn of my computer.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

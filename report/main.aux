\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {I}INTRODUCTION}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Feature Design}{1}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-A}Removing Noisy Backgrounds}{1}{subsection.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Three sample images from EMNIST, before filling the backgrounds (top) and after filling the backgrounds (bottom).}}{1}{figure.3}}
\newlabel{figurelabel}{{3}{1}{Three sample images from EMNIST, before filling the backgrounds (top) and after filling the backgrounds (bottom)}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-B}Extracting the 'Largest' Digit}{1}{subsection.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Three sample images from the MNIST dataset.}}{2}{figure.1}}
\newlabel{fig:test1}{{1}{2}{Three sample images from the MNIST dataset}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Three sample images from the EMNIST dataset.}}{2}{figure.2}}
\newlabel{fig:test2}{{2}{2}{Three sample images from the EMNIST dataset}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A sample image from EMNIST, superimposed by the geometric objects used to measure the size of each image using heuristics 1-4. Heuristic 1 is the top left, Huerustic 2 is the top right, Heuristic 3 is the bottom left, and Heuristic 4 is the bottom right.}}{2}{figure.4}}
\newlabel{figurelabel}{{4}{2}{A sample image from EMNIST, superimposed by the geometric objects used to measure the size of each image using heuristics 1-4. Heuristic 1 is the top left, Huerustic 2 is the top right, Heuristic 3 is the bottom left, and Heuristic 4 is the bottom right}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-C}Data Augmentation}{2}{subsection.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {III}ALGORITHMS}{2}{section.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces One of the four CNN archetectures we experimented with. The other three differ slightly in the kernel size, pooling size, and number of convolutional layers. Image modified from [4]. }}{3}{figure.5}}
\newlabel{figurelabel}{{5}{3}{One of the four CNN archetectures we experimented with. The other three differ slightly in the kernel size, pooling size, and number of convolutional layers. Image modified from [4]}{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}METHODOLOGY}{3}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-A}Logistic Regression: Model Tuning}{3}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-B}Feedforward Neural Network: Model Tuning}{3}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-C}CNN: Hyperparameter Tuning}{3}{subsection.4.3}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Hyperparameter Ranges Explored for CNN models}}{3}{table.1}}
\newlabel{my-label}{{I}{3}{Hyperparameter Ranges Explored for CNN models}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Constrained Hyperparameter Ranges}}{3}{table.2}}
\newlabel{my-label}{{II}{3}{Constrained Hyperparameter Ranges}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-D}CNN: Learning Optimization}{3}{subsection.4.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-E}CNN: Aggregated Predictions}{3}{subsection.4.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Learning curves for training periods with $\eta = .0001$ and $\eta = .00005$, with early stopping. Note: training accuracy is lower than validation accuracy. This is because we used data augmentation, and the new randomly rotated images made the training data "harder" to learn than the validation data.}}{4}{figure.6}}
\newlabel{figurelabel}{{6}{4}{Learning curves for training periods with $\eta = .0001$ and $\eta = .00005$, with early stopping. Note: training accuracy is lower than validation accuracy. This is because we used data augmentation, and the new randomly rotated images made the training data "harder" to learn than the validation data}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-F}Tf-Idf Features}{4}{subsection.4.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-G}Question Classifcation Features}{5}{subsection.4.7}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Architecture details of LSTM question classifiers }}{5}{table.3}}
\newlabel{my-label}{{III}{5}{Architecture details of LSTM question classifiers}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-H}Sentence Semantic Similarity Scores}{5}{subsection.4.8}}
\@writefile{toc}{\contentsline {section}{\numberline {V}RESULTS}{6}{section.5}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}DISCUSSION AND CONCLUSIONS}{6}{section.6}}
\bibcite{c1}{1}
\bibcite{c2}{2}
\bibcite{c3}{3}
\bibcite{c5}{4}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Test set accuracy scores, with different feature sets omitted}}{7}{table.4}}
\newlabel{my-label}{{IV}{7}{Test set accuracy scores, with different feature sets omitted}{table.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Variable importance chart of model trained on all features.}}{7}{figure.7}}
\newlabel{fig:test1}{{7}{7}{Variable importance chart of model trained on all features}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Variable importance chart of model that excludes the Tf-Idf features.}}{7}{figure.8}}
\newlabel{fig:test2}{{8}{7}{Variable importance chart of model that excludes the Tf-Idf features}{figure.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}STATEMENT OF CONTRIBUTION}{7}{section.7}}
\@writefile{toc}{\contentsline {section}{\numberline {VIII}APPENDIX}{7}{section.8}}
\@writefile{toc}{\contentsline {section}{References}{7}{section*.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Architecture 1: Three convolutional layers, 76,978 trainable parameters.}}{8}{figure.9}}
\newlabel{figurelabel}{{9}{8}{Architecture 1: Three convolutional layers, 76,978 trainable parameters}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Architecture 2: Four convolutional layers, 79,298 trainable parameters.}}{8}{figure.10}}
\newlabel{figurelabel}{{10}{8}{Architecture 2: Four convolutional layers, 79,298 trainable parameters}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Architecture 3: Four convolutional layers, 307,778 trainable parameters.}}{9}{figure.11}}
\newlabel{figurelabel}{{11}{9}{Architecture 3: Four convolutional layers, 307,778 trainable parameters}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Architecture 4: Six convolutional layers, 94,594 trainable parameters.}}{9}{figure.12}}
\newlabel{figurelabel}{{12}{9}{Architecture 4: Six convolutional layers, 94,594 trainable parameters}{figure.12}{}}
